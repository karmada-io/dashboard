# Karmada-Manager åç«¯å®ç°æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£åŸºäºç°æœ‰çš„ `pkg` ç›®å½•ä»£ç ç»“æ„ï¼Œæä¾›å…·ä½“çš„å®ç°æŒ‡å—ã€‚éµå¾ªç°æœ‰çš„æ¶æ„æ¨¡å¼ï¼Œå®ç°å±‚æ¬¡åŒ–ä¿¡æ¯æ±‡æ€»ã€ç²¾ç¡®é›†ç¾¤ç®¡ç†å’Œè°ƒåº¦å¯è§†åŒ–åŠŸèƒ½ã€‚

## ç°æœ‰ä»£ç æ¨¡å¼åˆ†æ

### 1. èµ„æºå®šä¹‰æ¨¡å¼
åŸºäº `pkg/resource/cluster/cluster.go` çš„ç°æœ‰æ¨¡å¼ï¼š

```go
// 1. å®šä¹‰èµ„æºç»“æ„ä½“
type Cluster struct {
    ObjectMeta         types.ObjectMeta          `json:"objectMeta"`
    TypeMeta           types.TypeMeta            `json:"typeMeta"`
    Ready              metav1.ConditionStatus    `json:"ready"`
    KubernetesVersion  string                    `json:"kubernetesVersion,omitempty"`
    SyncMode           v1alpha1.ClusterSyncMode  `json:"syncMode"`
    NodeSummary        *v1alpha1.NodeSummary     `json:"nodeSummary,omitempty"`
    AllocatedResources ClusterAllocatedResources `json:"allocatedResources"`
}

// 2. å®šä¹‰åˆ—è¡¨ç»“æ„ä½“
type ClusterList struct {
    ListMeta types.ListMeta `json:"listMeta"`
    Clusters []Cluster      `json:"clusters"`
    Errors   []error        `json:"errors"`
}

// 3. ä¸»è¦è·å–å‡½æ•°
func GetClusterList(client karmadaclientset.Interface, dsQuery *dataselect.DataSelectQuery) (*ClusterList, error)
```

### 2. è·¯ç”±å¤„ç†æ¨¡å¼
åŸºäº `cmd/api/app/routes/member/namespace/handler.go` çš„ç°æœ‰æ¨¡å¼ï¼š

```go
func handleGetMemberNamespace(c *gin.Context) {
    // 1. è·å–æˆå‘˜é›†ç¾¤å®¢æˆ·ç«¯
    memberClient := client.InClusterClientForMemberCluster(c.Param("clustername"))
    
    // 2. è§£ææŸ¥è¯¢å‚æ•°
    dataSelect := common.ParseDataSelectPathParameter(c)
    
    // 3. è°ƒç”¨èµ„æºå±‚è·å–æ•°æ®
    result, err := ns.GetNamespaceList(memberClient, dataSelect)
    if err != nil {
        common.Fail(c, err)
        return
    }
    
    // 4. è¿”å›æˆåŠŸå“åº”
    common.Success(c, result)
}

func init() {
    r := router.MemberV1()
    r.GET("/namespace", handleGetMemberNamespace)
}
```

## æ ¸å¿ƒåŠŸèƒ½å®ç°

### 1. èŠ‚ç‚¹ç®¡ç† API å®ç°

#### pkg/resource/node/enhanced_node.go
```go
/*
Copyright 2024 The Karmada Authors.
Licensed under the Apache License, Version 2.0
*/

package node

import (
    "context"
    "fmt"

    v1 "k8s.io/api/core/v1"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/kubernetes"

    "github.com/karmada-io/dashboard/pkg/common/helpers"
    "github.com/karmada-io/dashboard/pkg/common/types"
    "github.com/karmada-io/dashboard/pkg/dataselect"
)

// EnhancedNode å¢å¼ºèŠ‚ç‚¹ä¿¡æ¯
type EnhancedNode struct {
    ObjectMeta      types.ObjectMeta    `json:"objectMeta"`
    TypeMeta        types.TypeMeta      `json:"typeMeta"`
    Status          v1.NodeStatus       `json:"status"`
    PodSummary      PodSummary          `json:"podSummary"`
    ResourceSummary ResourceSummary     `json:"resourceSummary"`
    ClusterName     string              `json:"clusterName"`
}

// EnhancedNodeList å¢å¼ºèŠ‚ç‚¹åˆ—è¡¨
type EnhancedNodeList struct {
    ListMeta types.ListMeta  `json:"listMeta"`
    Nodes    []EnhancedNode  `json:"nodes"`
    Errors   []error         `json:"errors"`
}

// PodSummary Podç»Ÿè®¡ä¿¡æ¯
type PodSummary struct {
    TotalCount   int `json:"totalCount"`
    RunningCount int `json:"runningCount"`
    PendingCount int `json:"pendingCount"`
    FailedCount  int `json:"failedCount"`
}

// ResourceSummary èµ„æºæ±‡æ€»ä¿¡æ¯
type ResourceSummary struct {
    CPU    ResourceInfo `json:"cpu"`
    Memory ResourceInfo `json:"memory"`
    Pods   ResourceInfo `json:"pods"`
}

// ResourceInfo èµ„æºä½¿ç”¨ä¿¡æ¯
type ResourceInfo struct {
    Capacity    string `json:"capacity"`
    Allocatable string `json:"allocatable"`
    Allocated   string `json:"allocated"`
    Utilization string `json:"utilization"`
}

// GetEnhancedNodeList è·å–å¢å¼ºçš„èŠ‚ç‚¹åˆ—è¡¨
func GetEnhancedNodeList(client kubernetes.Interface, clusterName string, dsQuery *dataselect.DataSelectQuery) (*EnhancedNodeList, error) {
    // è·å–èŠ‚ç‚¹åˆ—è¡¨
    nodes, err := client.CoreV1().Nodes().List(context.TODO(), helpers.ListEverything)
    if err != nil {
        return nil, fmt.Errorf("failed to list nodes: %w", err)
    }

    // è½¬æ¢ä¸ºå¢å¼ºèŠ‚ç‚¹åˆ—è¡¨
    return toEnhancedNodeList(client, clusterName, nodes.Items, dsQuery)
}

// GetEnhancedNodeDetail è·å–å¢å¼ºçš„èŠ‚ç‚¹è¯¦æƒ…
func GetEnhancedNodeDetail(client kubernetes.Interface, clusterName, nodeName string) (*EnhancedNode, error) {
    // è·å–èŠ‚ç‚¹è¯¦æƒ…
    node, err := client.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions{})
    if err != nil {
        return nil, fmt.Errorf("failed to get node %s: %w", nodeName, err)
    }

    // è½¬æ¢ä¸ºå¢å¼ºèŠ‚ç‚¹
    return toEnhancedNode(client, clusterName, node)
}

// GetPodsOnNode è·å–èŠ‚ç‚¹ä¸Šçš„Podåˆ—è¡¨
func GetPodsOnNode(client kubernetes.Interface, nodeName string, dsQuery *dataselect.DataSelectQuery) (*PodList, error) {
    // è·å–èŠ‚ç‚¹ä¸Šçš„Pod
    fieldSelector := fmt.Sprintf("spec.nodeName=%s", nodeName)
    pods, err := client.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{
        FieldSelector: fieldSelector,
    })
    if err != nil {
        return nil, fmt.Errorf("failed to get pods on node %s: %w", nodeName, err)
    }

    // è½¬æ¢ä¸ºPodåˆ—è¡¨ (ä½¿ç”¨ç°æœ‰çš„podåŒ…åŠŸèƒ½)
    return toPodList(pods.Items, dsQuery), nil
}

func toEnhancedNodeList(client kubernetes.Interface, clusterName string, nodes []v1.Node, dsQuery *dataselect.DataSelectQuery) (*EnhancedNodeList, error) {
    enhancedNodes := make([]EnhancedNode, 0, len(nodes))
    
    for _, node := range nodes {
        enhancedNode, err := toEnhancedNode(client, clusterName, &node)
        if err != nil {
            // è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹
            continue
        }
        enhancedNodes = append(enhancedNodes, *enhancedNode)
    }

    // åº”ç”¨æ•°æ®é€‰æ‹©æŸ¥è¯¢
    nodeCells := make([]dataselect.DataCell, len(enhancedNodes))
    for i, node := range enhancedNodes {
        nodeCells[i] = EnhancedNodeCell{node}
    }

    filteredCells, filteredTotal := dataselect.GenericDataSelectWithFilter(nodeCells, dsQuery)
    filteredNodes := make([]EnhancedNode, len(filteredCells))
    for i, cell := range filteredCells {
        filteredNodes[i] = cell.(EnhancedNodeCell).EnhancedNode
    }

    return &EnhancedNodeList{
        ListMeta: types.ListMeta{TotalItems: filteredTotal},
        Nodes:    filteredNodes,
        Errors:   []error{},
    }, nil
}

func toEnhancedNode(client kubernetes.Interface, clusterName string, node *v1.Node) (*EnhancedNode, error) {
    // è·å–Podæ±‡æ€»ä¿¡æ¯
    podSummary, err := getPodSummaryForNode(client, node.Name)
    if err != nil {
        return nil, fmt.Errorf("failed to get pod summary for node %s: %w", node.Name, err)
    }

    // è·å–èµ„æºæ±‡æ€»ä¿¡æ¯
    resourceSummary, err := getResourceSummaryForNode(client, node)
    if err != nil {
        return nil, fmt.Errorf("failed to get resource summary for node %s: %w", node.Name, err)
    }

    return &EnhancedNode{
        ObjectMeta:      types.NewObjectMeta(node.ObjectMeta),
        TypeMeta:        types.NewTypeMeta(types.ResourceKindNode),
        Status:          node.Status,
        PodSummary:      podSummary,
        ResourceSummary: resourceSummary,
        ClusterName:     clusterName,
    }, nil
}

func getPodSummaryForNode(client kubernetes.Interface, nodeName string) (PodSummary, error) {
    fieldSelector := fmt.Sprintf("spec.nodeName=%s", nodeName)
    pods, err := client.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{
        FieldSelector: fieldSelector,
    })
    if err != nil {
        return PodSummary{}, err
    }

    summary := PodSummary{
        TotalCount: len(pods.Items),
    }

    for _, pod := range pods.Items {
        switch pod.Status.Phase {
        case v1.PodRunning:
            summary.RunningCount++
        case v1.PodPending:
            summary.PendingCount++
        case v1.PodFailed:
            summary.FailedCount++
        }
    }

    return summary, nil
}

func getResourceSummaryForNode(client kubernetes.Interface, node *v1.Node) (ResourceSummary, error) {
    // è·å–èŠ‚ç‚¹ä¸Šçš„Podæ¥è®¡ç®—å·²åˆ†é…èµ„æº
    fieldSelector := fmt.Sprintf("spec.nodeName=%s", node.Name)
    pods, err := client.CoreV1().Pods("").List(context.TODO(), metav1.ListOptions{
        FieldSelector: fieldSelector,
    })
    if err != nil {
        return ResourceSummary{}, err
    }

    // è®¡ç®—å·²åˆ†é…èµ„æº
    allocatedCPU := int64(0)
    allocatedMemory := int64(0)
    runningPods := int64(0)

    for _, pod := range pods.Items {
        if pod.Status.Phase == v1.PodRunning || pod.Status.Phase == v1.PodPending {
            runningPods++
        }

        for _, container := range pod.Spec.Containers {
            if cpu := container.Resources.Requests.Cpu(); cpu != nil {
                allocatedCPU += cpu.MilliValue()
            }
            if memory := container.Resources.Requests.Memory(); memory != nil {
                allocatedMemory += memory.Value()
            }
        }
    }

    // è·å–èŠ‚ç‚¹å®¹é‡
    capacity := node.Status.Capacity
    allocatable := node.Status.Allocatable

    // è®¡ç®—åˆ©ç”¨ç‡
    cpuUtilization := calculateCPUUtilization(allocatedCPU, allocatable.Cpu().MilliValue())
    memoryUtilization := calculateMemoryUtilization(allocatedMemory, allocatable.Memory().Value())
    podUtilization := calculatePodUtilization(runningPods, allocatable.Pods().Value())

    return ResourceSummary{
        CPU: ResourceInfo{
            Capacity:    capacity.Cpu().String(),
            Allocatable: allocatable.Cpu().String(),
            Allocated:   fmt.Sprintf("%dm", allocatedCPU),
            Utilization: fmt.Sprintf("%.1f%%", cpuUtilization),
        },
        Memory: ResourceInfo{
            Capacity:    capacity.Memory().String(),
            Allocatable: allocatable.Memory().String(),
            Allocated:   fmt.Sprintf("%d", allocatedMemory),
            Utilization: fmt.Sprintf("%.1f%%", memoryUtilization),
        },
        Pods: ResourceInfo{
            Capacity:    capacity.Pods().String(),
            Allocatable: allocatable.Pods().String(),
            Allocated:   fmt.Sprintf("%d", runningPods),
            Utilization: fmt.Sprintf("%.1f%%", podUtilization),
        },
    }, nil
}

// è®¡ç®—åˆ©ç”¨ç‡çš„è¾…åŠ©å‡½æ•°
func calculateCPUUtilization(allocated, allocatable int64) float64 {
    if allocatable == 0 {
        return 0
    }
    return float64(allocated) / float64(allocatable) * 100
}

func calculateMemoryUtilization(allocated, allocatable int64) float64 {
    if allocatable == 0 {
        return 0
    }
    return float64(allocated) / float64(allocatable) * 100
}

func calculatePodUtilization(allocated, allocatable int64) float64 {
    if allocatable == 0 {
        return 0
    }
    return float64(allocated) / float64(allocatable) * 100
}

// EnhancedNodeCell å®ç°DataCellæ¥å£
type EnhancedNodeCell struct {
    EnhancedNode EnhancedNode
}

func (cell EnhancedNodeCell) GetProperty(name dataselect.PropertyName) dataselect.ComparableValue {
    switch name {
    case dataselect.NameProperty:
        return dataselect.StdComparableString(cell.EnhancedNode.ObjectMeta.Name)
    case dataselect.CreationTimestampProperty:
        return dataselect.StdComparableTime(cell.EnhancedNode.ObjectMeta.CreationTimestamp.Time)
    default:
        return nil
    }
}
```

#### cmd/api/app/routes/member/node/handler.go
```go
/*
Copyright 2024 The Karmada Authors.
Licensed under the Apache License, Version 2.0
*/

package node

import (
    "github.com/gin-gonic/gin"

    "github.com/karmada-io/dashboard/cmd/api/app/router"
    "github.com/karmada-io/dashboard/cmd/api/app/types/common"
    "github.com/karmada-io/dashboard/pkg/client"
    enhancednode "github.com/karmada-io/dashboard/pkg/resource/node"
)

func handleGetMemberNodes(c *gin.Context) {
    clusterName := c.Param("clustername")
    memberClient := client.InClusterClientForMemberCluster(clusterName)
    if memberClient == nil {
        common.Fail(c, fmt.Errorf("failed to get client for cluster %s", clusterName))
        return
    }

    dataSelect := common.ParseDataSelectPathParameter(c)
    result, err := enhancednode.GetEnhancedNodeList(memberClient, clusterName, dataSelect)
    if err != nil {
        common.Fail(c, err)
        return
    }
    common.Success(c, result)
}

func handleGetMemberNodeDetail(c *gin.Context) {
    clusterName := c.Param("clustername")
    nodeName := c.Param("name")
    
    memberClient := client.InClusterClientForMemberCluster(clusterName)
    if memberClient == nil {
        common.Fail(c, fmt.Errorf("failed to get client for cluster %s", clusterName))
        return
    }

    result, err := enhancednode.GetEnhancedNodeDetail(memberClient, clusterName, nodeName)
    if err != nil {
        common.Fail(c, err)
        return
    }
    common.Success(c, result)
}

func handleGetMemberNodePods(c *gin.Context) {
    clusterName := c.Param("clustername")
    nodeName := c.Param("name")
    
    memberClient := client.InClusterClientForMemberCluster(clusterName)
    if memberClient == nil {
        common.Fail(c, fmt.Errorf("failed to get client for cluster %s", clusterName))
        return
    }

    dataSelect := common.ParseDataSelectPathParameter(c)
    result, err := enhancednode.GetPodsOnNode(memberClient, nodeName, dataSelect)
    if err != nil {
        common.Fail(c, err)
        return
    }
    common.Success(c, result)
}

func init() {
    r := router.MemberV1()
    r.GET("/nodes", handleGetMemberNodes)
    r.GET("/nodes/:name", handleGetMemberNodeDetail)
    r.GET("/nodes/:name/pods", handleGetMemberNodePods)
}
```

### 2. è°ƒåº¦ä¿¡æ¯ API å®ç°

#### pkg/resource/scheduling/workload_scheduling.go
```go
/*
Copyright 2024 The Karmada Authors.
Licensed under the Apache License, Version 2.0
*/

package scheduling

import (
    "context"
    "fmt"

    "github.com/karmada-io/karmada/pkg/apis/policy/v1alpha1"
    workv1alpha1 "github.com/karmada-io/karmada/pkg/apis/work/v1alpha1"
    karmadaclientset "github.com/karmada-io/karmada/pkg/generated/clientset/versioned"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

    "github.com/karmada-io/dashboard/pkg/common/types"
)

// WorkloadSchedulingView å·¥ä½œè´Ÿè½½è°ƒåº¦è§†å›¾
type WorkloadSchedulingView struct {
    WorkloadInfo      WorkloadInfo        `json:"workloadInfo"`
    PropagationPolicy *PolicyInfo         `json:"propagationPolicy,omitempty"`
    OverridePolicy    *PolicyInfo         `json:"overridePolicy,omitempty"`
    ClusterPlacements []ClusterPlacement  `json:"clusterPlacements"`
    SchedulingStatus  SchedulingStatus    `json:"schedulingStatus"`
}

// WorkloadInfo å·¥ä½œè´Ÿè½½åŸºæœ¬ä¿¡æ¯
type WorkloadInfo struct {
    Name           string `json:"name"`
    Namespace      string `json:"namespace"`
    Kind           string `json:"kind"`
    APIVersion     string `json:"apiVersion"`
    Replicas       int32  `json:"replicas"`
    ReadyReplicas  int32  `json:"readyReplicas"`
}

// PolicyInfo ç­–ç•¥ä¿¡æ¯
type PolicyInfo struct {
    Name            string                         `json:"name"`
    Namespace       string                         `json:"namespace"`
    ClusterAffinity *v1alpha1.ClusterAffinity     `json:"clusterAffinity,omitempty"`
    Placement       *v1alpha1.Placement           `json:"placement,omitempty"`
}

// ClusterPlacement é›†ç¾¤è°ƒåº¦ä¿¡æ¯
type ClusterPlacement struct {
    ClusterName     string `json:"clusterName"`
    PlannedReplicas int32  `json:"plannedReplicas"`
    ActualReplicas  int32  `json:"actualReplicas"`
    Weight          int32  `json:"weight,omitempty"`
    Reason          string `json:"reason"`
}

// SchedulingStatus è°ƒåº¦çŠ¶æ€
type SchedulingStatus struct {
    Phase   string `json:"phase"`   // Scheduled, Pending, Failed
    Message string `json:"message"`
}

// GetWorkloadScheduling è·å–å·¥ä½œè´Ÿè½½è°ƒåº¦ä¿¡æ¯
func GetWorkloadScheduling(karmadaClient karmadaclientset.Interface, namespace, name, kind string) (*WorkloadSchedulingView, error) {
    // 1. è·å–å·¥ä½œè´Ÿè½½åŸºæœ¬ä¿¡æ¯
    workloadInfo, err := getWorkloadInfo(karmadaClient, namespace, name, kind)
    if err != nil {
        return nil, fmt.Errorf("failed to get workload info: %w", err)
    }

    // 2. æŸ¥æ‰¾å…³è”çš„PropagationPolicy
    propagationPolicy, err := findAssociatedPropagationPolicy(karmadaClient, namespace, name, kind)
    if err != nil {
        // ç­–ç•¥ä¸å­˜åœ¨ä¸æ˜¯è‡´å‘½é”™è¯¯
        propagationPolicy = nil
    }

    // 3. æŸ¥æ‰¾å…³è”çš„OverridePolicy
    overridePolicy, err := findAssociatedOverridePolicy(karmadaClient, namespace, name, kind)
    if err != nil {
        // ç­–ç•¥ä¸å­˜åœ¨ä¸æ˜¯è‡´å‘½é”™è¯¯
        overridePolicy = nil
    }

    // 4. è·å–ResourceBindingä¿¡æ¯
    clusterPlacements, status, err := getClusterPlacementsFromBinding(karmadaClient, namespace, name, kind)
    if err != nil {
        return nil, fmt.Errorf("failed to get cluster placements: %w", err)
    }

    return &WorkloadSchedulingView{
        WorkloadInfo:      *workloadInfo,
        PropagationPolicy: propagationPolicy,
        OverridePolicy:    overridePolicy,
        ClusterPlacements: clusterPlacements,
        SchedulingStatus:  status,
    }, nil
}

func getWorkloadInfo(karmadaClient karmadaclientset.Interface, namespace, name, kind string) (*WorkloadInfo, error) {
    // æ ¹æ®ä¸åŒçš„workloadç±»å‹è·å–ä¿¡æ¯
    // è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ ¹æ®kindè°ƒç”¨ä¸åŒçš„API
    
    // è·å–ResourceTemplateæ¥è·å–å·¥ä½œè´Ÿè½½ä¿¡æ¯
    resourceTemplates, err := karmadaClient.WorkV1alpha1().ResourceBindings(namespace).List(context.TODO(), metav1.ListOptions{
        LabelSelector: fmt.Sprintf("resourcebinding.karmada.io/name=%s", name),
    })
    if err != nil {
        return nil, err
    }

    if len(resourceTemplates.Items) == 0 {
        return nil, fmt.Errorf("no resource binding found for %s/%s", namespace, name)
    }

    // ä»ResourceBindingä¸­æå–å·¥ä½œè´Ÿè½½ä¿¡æ¯
    binding := resourceTemplates.Items[0]
    
    return &WorkloadInfo{
        Name:       name,
        Namespace:  namespace,
        Kind:       kind,
        APIVersion: "apps/v1", // ç®€åŒ–å¤„ç†
        // å‰¯æœ¬æ•°éœ€è¦ä»å®é™…èµ„æºä¸­è·å–
        Replicas:      calculateTotalReplicas(binding.Spec.Clusters),
        ReadyReplicas: calculateReadyReplicas(binding.Status.AggregatedStatus),
    }, nil
}

func findAssociatedPropagationPolicy(karmadaClient karmadaclientset.Interface, namespace, name, kind string) (*PolicyInfo, error) {
    // è·å–æ‰€æœ‰PropagationPolicy
    policies, err := karmadaClient.PolicyV1alpha1().PropagationPolicies(namespace).List(context.TODO(), metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    // æŸ¥æ‰¾åŒ¹é…çš„ç­–ç•¥
    for _, policy := range policies.Items {
        if isPolicyMatchingWorkload(&policy.Spec.ResourceSelectors, namespace, name, kind) {
            return &PolicyInfo{
                Name:            policy.Name,
                Namespace:       policy.Namespace,
                ClusterAffinity: &policy.Spec.Placement.ClusterAffinity,
                Placement:       &policy.Spec.Placement,
            }, nil
        }
    }

    return nil, fmt.Errorf("no matching propagation policy found")
}

func findAssociatedOverridePolicy(karmadaClient karmadaclientset.Interface, namespace, name, kind string) (*PolicyInfo, error) {
    // è·å–æ‰€æœ‰OverridePolicy
    policies, err := karmadaClient.PolicyV1alpha1().OverridePolicies(namespace).List(context.TODO(), metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    // æŸ¥æ‰¾åŒ¹é…çš„ç­–ç•¥
    for _, policy := range policies.Items {
        if isPolicyMatchingWorkload(&policy.Spec.ResourceSelectors, namespace, name, kind) {
            return &PolicyInfo{
                Name:      policy.Name,
                Namespace: policy.Namespace,
            }, nil
        }
    }

    return nil, fmt.Errorf("no matching override policy found")
}

func getClusterPlacementsFromBinding(karmadaClient karmadaclientset.Interface, namespace, name, kind string) ([]ClusterPlacement, SchedulingStatus, error) {
    // è·å–ResourceBinding
    bindings, err := karmadaClient.WorkV1alpha1().ResourceBindings(namespace).List(context.TODO(), metav1.ListOptions{
        LabelSelector: fmt.Sprintf("resourcebinding.karmada.io/name=%s", name),
    })
    if err != nil {
        return nil, SchedulingStatus{Phase: "Failed", Message: err.Error()}, err
    }

    if len(bindings.Items) == 0 {
        return nil, SchedulingStatus{Phase: "Pending", Message: "No resource binding found"}, nil
    }

    binding := bindings.Items[0]
    placements := make([]ClusterPlacement, 0, len(binding.Spec.Clusters))

    for _, cluster := range binding.Spec.Clusters {
        placement := ClusterPlacement{
            ClusterName:     cluster.Name,
            PlannedReplicas: cluster.Replicas,
            ActualReplicas:  getActualReplicasFromStatus(binding.Status.AggregatedStatus, cluster.Name),
            Reason:          generatePlacementReason(cluster),
        }
        placements = append(placements, placement)
    }

    status := SchedulingStatus{
        Phase:   determineSchedulingPhase(binding.Status),
        Message: generateSchedulingMessage(binding.Status),
    }

    return placements, status, nil
}

// è¾…åŠ©å‡½æ•°
func isPolicyMatchingWorkload(resourceSelectors *[]v1alpha1.ResourceSelector, namespace, name, kind string) bool {
    if resourceSelectors == nil {
        return false
    }

    for _, selector := range *resourceSelectors {
        if selector.APIVersion == "apps/v1" && selector.Kind == kind {
            if selector.Namespace != "" && selector.Namespace != namespace {
                continue
            }
            if selector.Name != "" && selector.Name != name {
                continue
            }
            return true
        }
    }
    return false
}

func calculateTotalReplicas(clusters []workv1alpha1.TargetCluster) int32 {
    total := int32(0)
    for _, cluster := range clusters {
        total += cluster.Replicas
    }
    return total
}

func calculateReadyReplicas(aggregatedStatus []workv1alpha1.AggregatedStatusItem) int32 {
    // ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦è§£æçŠ¶æ€
    return 0
}

func getActualReplicasFromStatus(aggregatedStatus []workv1alpha1.AggregatedStatusItem, clusterName string) int32 {
    // ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦ä»çŠ¶æ€ä¸­æå–
    return 0
}

func generatePlacementReason(cluster workv1alpha1.TargetCluster) string {
    return fmt.Sprintf("æ ¹æ®è°ƒåº¦ç­–ç•¥åˆ†é… %d ä¸ªå‰¯æœ¬", cluster.Replicas)
}

func determineSchedulingPhase(status workv1alpha1.ResourceBindingStatus) string {
    // ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦åˆ†ææ¡ä»¶
    return "Scheduled"
}

func generateSchedulingMessage(status workv1alpha1.ResourceBindingStatus) string {
    return "æ‰€æœ‰å‰¯æœ¬éƒ½å·²æˆåŠŸè°ƒåº¦åˆ°ç›®æ ‡é›†ç¾¤"
}
```

#### cmd/api/app/routes/scheduling/handler.go
```go
/*
Copyright 2024 The Karmada Authors.
Licensed under the Apache License, Version 2.0
*/

package scheduling

import (
    "github.com/gin-gonic/gin"

    "github.com/karmada-io/dashboard/cmd/api/app/router"
    "github.com/karmada-io/dashboard/cmd/api/app/types/common"
    "github.com/karmada-io/dashboard/pkg/client"
    schedulingpkg "github.com/karmada-io/dashboard/pkg/resource/scheduling"
)

func handleGetWorkloadScheduling(c *gin.Context) {
    namespace := c.Param("namespace")
    name := c.Param("name")
    kind := c.Query("kind") // ä»æŸ¥è¯¢å‚æ•°è·å–kind
    
    if kind == "" {
        kind = "Deployment" // é»˜è®¤å€¼
    }

    karmadaClient := client.InClusterClient()
    result, err := schedulingpkg.GetWorkloadScheduling(karmadaClient, namespace, name, kind)
    if err != nil {
        common.Fail(c, err)
        return
    }
    common.Success(c, result)
}

func init() {
    r := router.V1()
    r.GET("/workloads/:namespace/:name/scheduling", handleGetWorkloadScheduling)
}
```

### 3. æ‰©å±•ç°æœ‰é›†ç¾¤ API

#### pkg/resource/cluster/enhanced_cluster.go
```go
/*
Copyright 2024 The Karmada Authors.
Licensed under the Apache License, Version 2.0
*/

package cluster

import (
    "context"
    "fmt"

    "github.com/karmada-io/karmada/pkg/apis/cluster/v1alpha1"
    karmadaclientset "github.com/karmada-io/karmada/pkg/generated/clientset/versioned"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

    "github.com/karmada-io/dashboard/pkg/client"
    "github.com/karmada-io/dashboard/pkg/common/types"
)

// GetClusterDetail è·å–é›†ç¾¤è¯¦ç»†ä¿¡æ¯
func GetClusterDetail(karmadaClient karmadaclientset.Interface, clusterName string) (*Cluster, error) {
    cluster, err := karmadaClient.ClusterV1alpha1().Clusters().Get(context.TODO(), clusterName, metav1.GetOptions{})
    if err != nil {
        return nil, fmt.Errorf("failed to get cluster %s: %w", clusterName, err)
    }

    return toClusterWithEnhancedInfo(cluster)
}

// UpdateClusterConfig æ›´æ–°é›†ç¾¤é…ç½®
func UpdateClusterConfig(karmadaClient karmadaclientset.Interface, clusterName string, updateFunc func(*v1alpha1.Cluster)) (*Cluster, error) {
    cluster, err := karmadaClient.ClusterV1alpha1().Clusters().Get(context.TODO(), clusterName, metav1.GetOptions{})
    if err != nil {
        return nil, fmt.Errorf("failed to get cluster %s: %w", clusterName, err)
    }

    // åº”ç”¨æ›´æ–°
    updateFunc(cluster)

    // æ›´æ–°é›†ç¾¤
    updatedCluster, err := karmadaClient.ClusterV1alpha1().Clusters().Update(context.TODO(), cluster, metav1.UpdateOptions{})
    if err != nil {
        return nil, fmt.Errorf("failed to update cluster %s: %w", clusterName, err)
    }

    return toClusterWithEnhancedInfo(updatedCluster)
}

func toClusterWithEnhancedInfo(cluster *v1alpha1.Cluster) (*Cluster, error) {
    // åŸºäºç°æœ‰çš„toClusterå‡½æ•°ï¼Œæ·»åŠ æ›´å¤šä¿¡æ¯
    baseCluster := toCluster(cluster)
    
    // æ·»åŠ å¢å¼ºä¿¡æ¯
    // å¯ä»¥ä»æˆå‘˜é›†ç¾¤è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
    memberClient := client.InClusterClientForMemberCluster(cluster.Name)
    if memberClient != nil {
        // è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯ç­‰
        enhanceWithMemberClusterInfo(&baseCluster, memberClient)
    }

    return &baseCluster, nil
}

func enhanceWithMemberClusterInfo(cluster *Cluster, memberClient client.Client) {
    // ä»æˆå‘˜é›†ç¾¤è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
    // ä¾‹å¦‚ï¼šå®æ—¶çš„èµ„æºä½¿ç”¨æƒ…å†µã€ç½‘ç»œçŠ¶æ€ç­‰
    // è¿™é‡Œç®€åŒ–å®ç°
}
```

## è·¯ç”±æ³¨å†Œæ±‡æ€»

### åœ¨ cmd/api/app/router/router.go ä¸­ç¡®ä¿æ­£ç¡®å¯¼å…¥
```go
import (
    // ç°æœ‰å¯¼å…¥...
    
    // æ–°å¢å¯¼å…¥
    _ "github.com/karmada-io/dashboard/cmd/api/app/routes/member/node"
    _ "github.com/karmada-io/dashboard/cmd/api/app/routes/scheduling"
)
```

## æµ‹è¯•ç¤ºä¾‹

### 1. æµ‹è¯•èŠ‚ç‚¹ API
```bash
# è·å–é›†ç¾¤èŠ‚ç‚¹åˆ—è¡¨
curl -X GET "http://localhost:8080/api/v1/member/cluster-1/nodes?page=1&limit=10"

# è·å–èŠ‚ç‚¹è¯¦æƒ…
curl -X GET "http://localhost:8080/api/v1/member/cluster-1/nodes/node-1"

# è·å–èŠ‚ç‚¹ä¸Šçš„Pod
curl -X GET "http://localhost:8080/api/v1/member/cluster-1/nodes/node-1/pods"
```

### 2. æµ‹è¯•è°ƒåº¦ API
```bash
# è·å–å·¥ä½œè´Ÿè½½è°ƒåº¦ä¿¡æ¯
curl -X GET "http://localhost:8080/api/v1/workloads/default/nginx-deployment/scheduling?kind=Deployment"
```

## å®ç°ä¼˜å…ˆçº§

### Phase 1 (ç«‹å³å®ç°)
1. âœ… èŠ‚ç‚¹ç®¡ç†API (åŸºç¡€åŠŸèƒ½)
2. âœ… æ‰©å±•é›†ç¾¤è¯¦æƒ…API
3. âœ… è°ƒåº¦ä¿¡æ¯API (åŸºç¡€ç‰ˆæœ¬)

### Phase 2 (æ€§èƒ½ä¼˜åŒ–)
1. ğŸ”„ æ·»åŠ ç¼“å­˜æœºåˆ¶
2. ğŸ”„ å¹¶å‘æ•°æ®èšåˆ
3. ğŸ”„ é”™è¯¯å¤„ç†ä¼˜åŒ–

### Phase 3 (é«˜çº§åŠŸèƒ½)
1. ğŸ”„ Podè°ƒåº¦è¿½æº¯
2. ğŸ”„ å®æ—¶ç›‘æ§
3. ğŸ”„ WebSocketæ”¯æŒ

è¿™ä¸ªå®ç°æŒ‡å—å®Œå…¨åŸºäºç°æœ‰çš„ä»£ç ç»“æ„å’Œæ¨¡å¼ï¼Œç¡®ä¿äº†ä¸ç°æœ‰ç³»ç»Ÿçš„å…¼å®¹æ€§ï¼ŒåŒæ—¶æä¾›äº†æ‚¨éœ€è¦çš„å±‚æ¬¡åŒ–ä¿¡æ¯æ±‡æ€»å’Œè°ƒåº¦å¯è§†åŒ–åŠŸèƒ½ã€‚ 